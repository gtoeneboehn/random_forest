{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Predicting Boston Housing Prices using Random Forest*\n",
    "#### Authors: Tom Sharp, Troy Sattgast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os library and the pandas library (aliased as pd)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please update the **username** variable with your username on your comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsattgast\\Documents\\PyScripts\\random_forest\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsattgast\\Documents\\PyScripts\\random_forest\n",
      "C:\\Users\\tsattgast\\Documents\\PyScripts\\random_forest\\data\\Boston Housing Prices.csv\n",
      "C:\\Users\\tsattgast\\Documents\\PyScripts\\random_forest\\data\\boston_data_dict.csv\n",
      "C:\\Users\\tsattgast\\Documents\\PyScripts\\random_forest\\images\n"
     ]
    }
   ],
   "source": [
    "username = \"tsattgast\"\n",
    "\n",
    "# Store the paths to frequently used files\n",
    "\n",
    "parent_path = r\"C:/Users/\" + str(username) + \"/Desktop/Demystifying_ML/random_forest/\"\n",
    "data_path = parent_path + \"data/Boston Housing Prices.csv\"\n",
    "data_dict_path = parent_path + \"data/boston_data_dict.csv\"\n",
    "image_path = parent_path + 'images/'\n",
    "\n",
    "# ORRRRRR!\n",
    "\n",
    "parent_path = os.getcwd()\n",
    "data_path = os.path.join(parent_path,  'data', 'Boston Housing Prices.csv')\n",
    "data_dict_path = os.path.join(parent_path, 'data', 'boston_data_dict.csv')\n",
    "image_path = os.path.join(parent_path, 'images')\n",
    "\n",
    "\n",
    "\n",
    "print(parent_path)\n",
    "print(data_path)\n",
    "print(data_dict_path)\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Import, Exploration, and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During any analysis it is always important to first examine your data. This involves looking at the data itself, the column names, and some summary statistics about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data using the pandas package. The data is stored in what is called a dataframe (similar to a spreadsheet)\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(num of rows, num of columns) =  (506, 17)\n"
     ]
    }
   ],
   "source": [
    "# Examine number of rows and columns \n",
    "\n",
    "print(\"(num of rows, num of columns) = \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         town  tract  longitude   latitude    crime  residential  industrial  \\\n",
      "0      Nahant   2011 -70.955002  42.255001  0.00632         18.0        2.31   \n",
      "1  Swampscott   2021 -70.949997  42.287498  0.02731          0.0        7.07   \n",
      "2  Swampscott   2022 -70.935997  42.283001  0.02729          0.0        7.07   \n",
      "\n",
      "  river    nox  rooms      older  distance  highway  tax    ptratio  lstat  \\\n",
      "0    no  0.538  6.575  65.199997    4.0900        1  296  15.300000   4.98   \n",
      "1    no  0.469  6.421  78.900002    4.9671        2  242  17.799999   9.14   \n",
      "2    no  0.469  7.185  61.099998    4.9671        2  242  17.799999   4.03   \n",
      "\n",
      "       cmedv  \n",
      "0  24.000000  \n",
      "1  21.600000  \n",
      "2  34.700001  \n"
     ]
    }
   ],
   "source": [
    "# That's a lot of rows. Let's just look at the first three columns of the data, instead of all of them\n",
    "\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['town', 'tract', 'longitude', 'latitude', 'crime', 'residential',\n",
      "       'industrial', 'river', 'nox', 'rooms', 'older', 'distance', 'highway',\n",
      "       'tax', 'ptratio', 'lstat', 'cmedv'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# We can list all the column names by calling the \"columns\" attribute of \"data\" \n",
    "# Def: Attribute - describes the data (an adjective)\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tract   longitude    latitude       crime  residential  \\\n",
      "count   506.000000  506.000000  506.000000  506.000000   506.000000   \n",
      "mean   2700.355731  -71.056389   42.216440    3.613524    11.363636   \n",
      "std    1380.036830    0.075405    0.061777    8.601545    23.322453   \n",
      "min       1.000000  -71.289497   42.029999    0.006320     0.000000   \n",
      "25%    1303.250000  -71.093226   42.180774    0.082045     0.000000   \n",
      "50%    3393.500000  -71.052902   42.218100    0.256510     0.000000   \n",
      "75%    3739.750000  -71.019625   42.252249    3.677083    12.500000   \n",
      "max    5082.000000  -70.809998   42.381000   88.976196   100.000000   \n",
      "\n",
      "       industrial         nox       rooms       older    distance     highway  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean    11.136779    0.554695    6.284634   68.574901    3.795043    9.549407   \n",
      "std      6.860353    0.115878    0.702617   28.148862    2.105710    8.707259   \n",
      "min      0.460000    0.385000    3.561000    2.900000    1.129600    1.000000   \n",
      "25%      5.190000    0.449000    5.885500   45.025000    2.100175    4.000000   \n",
      "50%      9.690000    0.538000    6.208500   77.500000    3.207450    5.000000   \n",
      "75%     18.100000    0.624000    6.623500   94.074999    5.188425   24.000000   \n",
      "max     27.740000    0.871000    8.780000  100.000000   12.126500   24.000000   \n",
      "\n",
      "              tax     ptratio       lstat       cmedv  \n",
      "count  506.000000  506.000000  506.000000  506.000000  \n",
      "mean   408.237154   18.455534   12.653063   22.528854  \n",
      "std    168.537116    2.164946    7.141062    9.182176  \n",
      "min    187.000000   12.600000    1.730000    5.000000  \n",
      "25%    279.000000   17.400000    6.950000   17.025000  \n",
      "50%    330.000000   19.050000   11.360000   21.200001  \n",
      "75%    666.000000   20.200001   16.954999   25.000000  \n",
      "max    711.000000   22.000000   37.970001   50.000000  \n"
     ]
    }
   ],
   "source": [
    "# We can view summary statistics about the data by calling the \"describe()\" method of \"data\"\n",
    "# Def: Method - take an action on the data (a verb)\n",
    "\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Col Name                                         Definition\n",
      "0          town                                       name of town\n",
      "1         tract                                       census tract\n",
      "2     longitude                               long of census tract\n",
      "3      latitude                                lat of census tract\n",
      "4         crime                      per capita crime rate by town\n",
      "5   residential  proportion of residential land zoned for lots ...\n",
      "6    industrial   proportion of non-retail business acres per town\n",
      "7         river  Charles River dummy variable (= 1 if tract bou...\n",
      "8           nox  nitric oxides concentration (parts per 10 mill...\n",
      "9         rooms               average number of rooms per dwelling\n",
      "10        older  proportion of owner-occupied units older than ...\n",
      "11     distance  weighted distances to five Boston employment c...\n",
      "12      highway          index of accessibility to radial highways\n",
      "13          tax           full-value property-tax rate per $10,000\n",
      "14      ptratio  the ratio of students to teachers in primary a...\n",
      "15        lstat  % of homeowners in the neighborhood considered...\n",
      "16        cmedv    Median value of owner-occupied homes in $1000's\n"
     ]
    }
   ],
   "source": [
    "#What do all these fields mean? Let's use the data dictionary to find out\n",
    "\n",
    "data_dict = pd.read_csv(data_dict_path)\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**This last value, *cmedv*, is what we would like to predict using a machine learning. Before we can predict, we need to make sure we clean the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tract  longitude   latitude    crime  residential  industrial  river  \\\n",
      "0   2011 -70.955002  42.255001  0.00632         18.0        2.31  False   \n",
      "1   2021 -70.949997  42.287498  0.02731          0.0        7.07  False   \n",
      "2   2022 -70.935997  42.283001  0.02729          0.0        7.07  False   \n",
      "3   2031 -70.928001  42.292999  0.03237          0.0        2.18  False   \n",
      "4   2032 -70.921997  42.298000  0.06905          0.0        2.18  False   \n",
      "\n",
      "     nox  rooms      older  distance  highway  tax    ptratio  lstat  \\\n",
      "0  0.538  6.575  65.199997    4.0900        1  296  15.300000   4.98   \n",
      "1  0.469  6.421  78.900002    4.9671        2  242  17.799999   9.14   \n",
      "2  0.469  7.185  61.099998    4.9671        2  242  17.799999   4.03   \n",
      "3  0.458  6.998  45.799999    6.0622        3  222  18.700001   2.94   \n",
      "4  0.458  7.147  54.200001    6.0622        3  222  18.700001   5.33   \n",
      "\n",
      "       cmedv   val_mean  \n",
      "0  24.000000  22.528854  \n",
      "1  21.600000  22.528854  \n",
      "2  34.700001  22.528854  \n",
      "3  33.400002  22.528854  \n",
      "4  36.200001  22.528854  \n"
     ]
    }
   ],
   "source": [
    "# Clean the data - do all of this at once \n",
    "\n",
    "data.fillna(0)\n",
    "data['river'].replace('no', False, inplace = True)\n",
    "data['river'].replace('yes', True, inplace = True)\n",
    "data.drop(['town'], axis = 1, inplace = True)\n",
    "data['val_mean'] = data['cmedv'].mean()\n",
    "\n",
    "#print(data)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tract</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>crime</th>\n",
       "      <th>residential</th>\n",
       "      <th>industrial</th>\n",
       "      <th>river</th>\n",
       "      <th>nox</th>\n",
       "      <th>rooms</th>\n",
       "      <th>older</th>\n",
       "      <th>distance</th>\n",
       "      <th>highway</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>cmedv</th>\n",
       "      <th>val_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>-70.955002</td>\n",
       "      <td>42.255001</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>False</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.199997</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>22.528854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>-70.949997</td>\n",
       "      <td>42.287498</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>False</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>22.528854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>-70.935997</td>\n",
       "      <td>42.283001</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>False</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.099998</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.700001</td>\n",
       "      <td>22.528854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2031</td>\n",
       "      <td>-70.928001</td>\n",
       "      <td>42.292999</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>False</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.400002</td>\n",
       "      <td>22.528854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2032</td>\n",
       "      <td>-70.921997</td>\n",
       "      <td>42.298000</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>False</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>22.528854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tract  longitude   latitude    crime  residential  industrial  river  \\\n",
       "0   2011 -70.955002  42.255001  0.00632         18.0        2.31  False   \n",
       "1   2021 -70.949997  42.287498  0.02731          0.0        7.07  False   \n",
       "2   2022 -70.935997  42.283001  0.02729          0.0        7.07  False   \n",
       "3   2031 -70.928001  42.292999  0.03237          0.0        2.18  False   \n",
       "4   2032 -70.921997  42.298000  0.06905          0.0        2.18  False   \n",
       "\n",
       "     nox  rooms      older  distance  highway  tax    ptratio  lstat  \\\n",
       "0  0.538  6.575  65.199997    4.0900        1  296  15.300000   4.98   \n",
       "1  0.469  6.421  78.900002    4.9671        2  242  17.799999   9.14   \n",
       "2  0.469  7.185  61.099998    4.9671        2  242  17.799999   4.03   \n",
       "3  0.458  6.998  45.799999    6.0622        3  222  18.700001   2.94   \n",
       "4  0.458  7.147  54.200001    6.0622        3  222  18.700001   5.33   \n",
       "\n",
       "       cmedv   val_mean  \n",
       "0  24.000000  22.528854  \n",
       "1  21.600000  22.528854  \n",
       "2  34.700001  22.528854  \n",
       "3  33.400002  22.528854  \n",
       "4  36.200001  22.528854  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side Note - In most applications of data science and ML, we wouldtake a closer look at cleaning the data.\n",
    "Data gathering and cleansing usually consumes +80% of the DS/ML process; however, this dataset happened to be extremely clean when it was retrieved from its source online...and we're pretty lazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decision Tree - The Building Block of Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the columns in X are referred to as the features, and the y value is referred to as the target.\n",
    "Since we have a target variable here, this is known as supervised learning.\n",
    "The split takes the data and splits it into a dataset for training (fitting) the model, and a subset to test the model\n",
    "Here we are doing an 80/20 split, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tract' 'longitude' 'latitude' 'crime' 'residential' 'industrial' 'river'\n",
      " 'nox' 'rooms' 'older' 'distance' 'highway' 'tax' 'ptratio' 'lstat'\n",
      " 'val_mean']\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "features = data.drop('cmedv', axis=1)\n",
    "print(features.columns.values)\n",
    "labels = data['cmedv']\n",
    "feature_list = list(features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tract  longitude   latitude     crime  residential  industrial  river  \\\n",
      "0     2011 -70.955002  42.255001   0.00632         18.0        2.31  False   \n",
      "1     2021 -70.949997  42.287498   0.02731          0.0        7.07  False   \n",
      "2     2022 -70.935997  42.283001   0.02729          0.0        7.07  False   \n",
      "3     2031 -70.928001  42.292999   0.03237          0.0        2.18  False   \n",
      "4     2032 -70.921997  42.298000   0.06905          0.0        2.18  False   \n",
      "5     2033 -70.916496  42.304001   0.02985          0.0        2.18  False   \n",
      "6     2041 -70.935997  42.297001   0.08829         12.5        7.87  False   \n",
      "7     2042 -70.937500  42.310001   0.14455         12.5        7.87  False   \n",
      "8     2043 -70.932999  42.312000   0.21124         12.5        7.87  False   \n",
      "9     2044 -70.929001  42.316002   0.17004         12.5        7.87  False   \n",
      "10    2045 -70.934998  42.316002   0.22489         12.5        7.87  False   \n",
      "11    2046 -70.944000  42.317001   0.11747         12.5        7.87  False   \n",
      "12    2047 -70.950996  42.306000   0.09378         12.5        7.87  False   \n",
      "13    2051 -70.964500  42.292000   0.62976          0.0        8.14  False   \n",
      "14    2052 -70.972000  42.286999   0.63796          0.0        8.14  False   \n",
      "15    2053 -70.976501  42.293999   0.62739          0.0        8.14  False   \n",
      "16    2054 -70.986999  42.298500   1.05393          0.0        8.14  False   \n",
      "17    2055 -70.977997  42.285000   0.78420          0.0        8.14  False   \n",
      "18    2056 -70.992500  42.282501   0.80271          0.0        8.14  False   \n",
      "19    2057 -70.987999  42.277599   0.72580          0.0        8.14  False   \n",
      "20    2058 -70.983498  42.277000   1.25179          0.0        8.14  False   \n",
      "21    2059 -70.982002  42.280998   0.85204          0.0        8.14  False   \n",
      "22    2060 -70.977501  42.278999   1.23247          0.0        8.14  False   \n",
      "23    2061 -70.973000  42.278999   0.98843          0.0        8.14  False   \n",
      "24    2062 -70.969299  42.281601   0.75026          0.0        8.14  False   \n",
      "25    2063 -70.963997  42.284000   0.84054          0.0        8.14  False   \n",
      "26    2064 -70.959702  42.286999   0.67191          0.0        8.14  False   \n",
      "27    2065 -70.959702  42.282501   0.95577          0.0        8.14  False   \n",
      "28    2066 -70.957001  42.279999   0.77299          0.0        8.14  False   \n",
      "29    2067 -70.950996  42.278000   1.00245          0.0        8.14  False   \n",
      "..     ...        ...        ...       ...          ...         ...    ...   \n",
      "476   1204 -71.056503  42.188000   4.87141          0.0       18.10  False   \n",
      "477   1205 -71.052803  42.192001  15.02340          0.0       18.10  False   \n",
      "478   1206 -71.055801  42.191299  10.23300          0.0       18.10  False   \n",
      "479   1207 -71.067001  42.194500  14.33370          0.0       18.10  False   \n",
      "480   1301 -71.100800  42.174000   5.82401          0.0       18.10  False   \n",
      "481   1302 -71.095001  42.173000   5.70818          0.0       18.10  False   \n",
      "482   1303 -71.089996  42.166500   5.73116          0.0       18.10  False   \n",
      "483   1304 -71.097504  42.160801   2.81838          0.0       18.10  False   \n",
      "484   1401 -71.080399  42.153999   2.37857          0.0       18.10  False   \n",
      "485   1402 -71.074997  42.145500   3.67367          0.0       18.10  False   \n",
      "486   1403 -71.071503  42.154999   5.69175          0.0       18.10  False   \n",
      "487   1404 -71.065002  42.160999   4.83567          0.0       18.10  False   \n",
      "488   1601 -71.018898  42.234402   0.15086          0.0       27.74  False   \n",
      "489   1602 -71.022797  42.233501   0.18337          0.0       27.74  False   \n",
      "490   1604 -71.024498  42.236801   0.20746          0.0       27.74  False   \n",
      "491   1605 -71.015999  42.238201   0.10574          0.0       27.74  False   \n",
      "492   1606 -71.029701  42.244701   0.11132          0.0       27.74  False   \n",
      "493   1701 -71.012497  42.246201   0.17331          0.0        9.69  False   \n",
      "494   1702 -71.012497  42.250000   0.27957          0.0        9.69  False   \n",
      "495   1703 -71.010498  42.254700   0.17899          0.0        9.69  False   \n",
      "496   1704 -71.001000  42.252499   0.28960          0.0        9.69  False   \n",
      "497   1705 -70.994698  42.249600   0.26838          0.0        9.69  False   \n",
      "498   1706 -71.004997  42.245499   0.23912          0.0        9.69  False   \n",
      "499   1707 -70.998497  42.243000   0.17783          0.0        9.69  False   \n",
      "500   1708 -70.991997  42.237999   0.22438          0.0        9.69  False   \n",
      "501   1801 -70.986000  42.231201   0.06263          0.0       11.93  False   \n",
      "502   1802 -70.990997  42.227501   0.04527          0.0       11.93  False   \n",
      "503   1803 -70.994797  42.226002   0.06076          0.0       11.93  False   \n",
      "504   1804 -70.987503  42.223999   0.10959          0.0       11.93  False   \n",
      "505   1805 -70.982498  42.221001   0.04741          0.0       11.93  False   \n",
      "\n",
      "       nox  rooms       older  distance  highway  tax    ptratio      lstat  \\\n",
      "0    0.538  6.575   65.199997    4.0900        1  296  15.300000   4.980000   \n",
      "1    0.469  6.421   78.900002    4.9671        2  242  17.799999   9.140000   \n",
      "2    0.469  7.185   61.099998    4.9671        2  242  17.799999   4.030000   \n",
      "3    0.458  6.998   45.799999    6.0622        3  222  18.700001   2.940000   \n",
      "4    0.458  7.147   54.200001    6.0622        3  222  18.700001   5.330000   \n",
      "5    0.458  6.430   58.700001    6.0622        3  222  18.700001   5.210000   \n",
      "6    0.524  6.012   66.599998    5.5605        5  311  15.200000  12.430000   \n",
      "7    0.524  6.172   96.099998    5.9505        5  311  15.200000  19.150000   \n",
      "8    0.524  5.631  100.000000    6.0821        5  311  15.200000  29.930000   \n",
      "9    0.524  6.004   85.900002    6.5921        5  311  15.200000  17.100000   \n",
      "10   0.524  6.377   94.300003    6.3467        5  311  15.200000  20.450001   \n",
      "11   0.524  6.009   82.900002    6.2267        5  311  15.200000  13.270001   \n",
      "12   0.524  5.889   39.000000    5.4509        5  311  15.200000  15.710000   \n",
      "13   0.538  5.949   61.799999    4.7075        4  307  21.000000   8.260000   \n",
      "14   0.538  6.096   84.500000    4.4619        4  307  21.000000  10.260000   \n",
      "15   0.538  5.834   56.500000    4.4986        4  307  21.000000   8.470000   \n",
      "16   0.538  5.935   29.299999    4.4986        4  307  21.000000   6.580000   \n",
      "17   0.538  5.990   81.699997    4.2579        4  307  21.000000  14.670000   \n",
      "18   0.538  5.456   36.599998    3.7965        4  307  21.000000  11.690000   \n",
      "19   0.538  5.727   69.500000    3.7965        4  307  21.000000  11.280000   \n",
      "20   0.538  5.570   98.099998    3.7979        4  307  21.000000  21.020001   \n",
      "21   0.538  5.965   89.199997    4.0123        4  307  21.000000  13.830000   \n",
      "22   0.538  6.142   91.699997    3.9769        4  307  21.000000  18.719999   \n",
      "23   0.538  5.813  100.000000    4.0952        4  307  21.000000  19.879999   \n",
      "24   0.538  5.924   94.099998    4.3996        4  307  21.000000  16.299999   \n",
      "25   0.538  5.599   85.699997    4.4546        4  307  21.000000  16.510000   \n",
      "26   0.538  5.813   90.300003    4.6820        4  307  21.000000  14.810000   \n",
      "27   0.538  6.047   88.800003    4.4534        4  307  21.000000  17.280001   \n",
      "28   0.538  6.495   94.400002    4.4547        4  307  21.000000  12.800000   \n",
      "29   0.538  6.674   87.300003    4.2390        4  307  21.000000  11.979999   \n",
      "..     ...    ...         ...       ...      ...  ...        ...        ...   \n",
      "476  0.614  6.484   93.599998    2.3053       24  666  20.200001  18.680000   \n",
      "477  0.614  5.304   97.300003    2.1007       24  666  20.200001  24.910000   \n",
      "478  0.614  6.185   96.699997    2.1705       24  666  20.200001  18.030001   \n",
      "479  0.614  6.229   88.000000    1.9512       24  666  20.200001  13.110000   \n",
      "480  0.532  6.242   64.699997    3.4242       24  666  20.200001  10.740000   \n",
      "481  0.532  6.750   74.900002    3.3317       24  666  20.200001   7.740000   \n",
      "482  0.532  7.061   77.000000    3.4106       24  666  20.200001   7.010000   \n",
      "483  0.532  5.762   40.299999    4.0983       24  666  20.200001  10.420000   \n",
      "484  0.583  5.871   41.900002    3.7240       24  666  20.200001  13.340000   \n",
      "485  0.583  6.312   51.900002    3.9917       24  666  20.200001  10.580000   \n",
      "486  0.583  6.114   79.800003    3.5459       24  666  20.200001  14.979999   \n",
      "487  0.583  5.905   53.200001    3.1523       24  666  20.200001  11.450000   \n",
      "488  0.609  5.454   92.699997    1.8209        4  711  20.100000  18.059999   \n",
      "489  0.609  5.414   98.300003    1.7554        4  711  20.100000  23.969999   \n",
      "490  0.609  5.093   98.000000    1.8226        4  711  20.100000  29.680000   \n",
      "491  0.609  5.983   98.800003    1.8681        4  711  20.100000  18.070000   \n",
      "492  0.609  5.983   83.500000    2.1099        4  711  20.100000  13.350000   \n",
      "493  0.585  5.707   54.000000    2.3817        6  391  19.200001  12.010000   \n",
      "494  0.585  5.926   42.599998    2.3817        6  391  19.200001  13.590000   \n",
      "495  0.585  5.670   28.799999    2.7986        6  391  19.200001  17.600000   \n",
      "496  0.585  5.390   72.900002    2.7986        6  391  19.200001  21.139999   \n",
      "497  0.585  5.794   70.599998    2.8927        6  391  19.200001  14.100000   \n",
      "498  0.585  6.019   65.300003    2.4091        6  391  19.200001  12.920000   \n",
      "499  0.585  5.569   73.500000    2.3999        6  391  19.200001  15.100000   \n",
      "500  0.585  6.027   79.699997    2.4982        6  391  19.200001  14.330000   \n",
      "501  0.573  6.593   69.099998    2.4786        1  273  21.000000   9.670000   \n",
      "502  0.573  6.120   76.699997    2.2875        1  273  21.000000   9.080000   \n",
      "503  0.573  6.976   91.000000    2.1675        1  273  21.000000   5.640000   \n",
      "504  0.573  6.794   89.300003    2.3889        1  273  21.000000   6.480000   \n",
      "505  0.573  6.030   80.800003    2.5050        1  273  21.000000   7.880000   \n",
      "\n",
      "      val_mean  \n",
      "0    22.528854  \n",
      "1    22.528854  \n",
      "2    22.528854  \n",
      "3    22.528854  \n",
      "4    22.528854  \n",
      "5    22.528854  \n",
      "6    22.528854  \n",
      "7    22.528854  \n",
      "8    22.528854  \n",
      "9    22.528854  \n",
      "10   22.528854  \n",
      "11   22.528854  \n",
      "12   22.528854  \n",
      "13   22.528854  \n",
      "14   22.528854  \n",
      "15   22.528854  \n",
      "16   22.528854  \n",
      "17   22.528854  \n",
      "18   22.528854  \n",
      "19   22.528854  \n",
      "20   22.528854  \n",
      "21   22.528854  \n",
      "22   22.528854  \n",
      "23   22.528854  \n",
      "24   22.528854  \n",
      "25   22.528854  \n",
      "26   22.528854  \n",
      "27   22.528854  \n",
      "28   22.528854  \n",
      "29   22.528854  \n",
      "..         ...  \n",
      "476  22.528854  \n",
      "477  22.528854  \n",
      "478  22.528854  \n",
      "479  22.528854  \n",
      "480  22.528854  \n",
      "481  22.528854  \n",
      "482  22.528854  \n",
      "483  22.528854  \n",
      "484  22.528854  \n",
      "485  22.528854  \n",
      "486  22.528854  \n",
      "487  22.528854  \n",
      "488  22.528854  \n",
      "489  22.528854  \n",
      "490  22.528854  \n",
      "491  22.528854  \n",
      "492  22.528854  \n",
      "493  22.528854  \n",
      "494  22.528854  \n",
      "495  22.528854  \n",
      "496  22.528854  \n",
      "497  22.528854  \n",
      "498  22.528854  \n",
      "499  22.528854  \n",
      "500  22.528854  \n",
      "501  22.528854  \n",
      "502  22.528854  \n",
      "503  22.528854  \n",
      "504  22.528854  \n",
      "505  22.528854  \n",
      "\n",
      "[506 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into trainnig (for learning) and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we run our model, let's check our accuracy from our best guess--using an average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test mean, our baseline for guessing, is 22.57\n",
      "9.94130140151155\n",
      "6.908447652548243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "test_mean = y_test.mean()\n",
    "print(\"The test mean, our baseline for guessing, is {:.2f}\".format(test_mean))\n",
    "\n",
    "baseline_preds = X_test[:, feature_list.index('val_mean')]\n",
    "\n",
    "# Baseline errors, and display average baseline error and rmse\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, baseline_preds)))\n",
    "print(mean_absolute_error(y_test, baseline_preds))\n",
    "\n",
    "#remove val_mean from features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sci-kit learn uses a 3 step process to train a ML model:\n",
    "1. Import the model of choice\n",
    "2. Instantiate the model\n",
    "3. Fit (train) the model to the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from sklearn import tree\n",
    "\n",
    "#instantiate \n",
    "decision_tree = tree.DecisionTreeRegressor(random_state = 8)\n",
    "\n",
    "#train/fit\n",
    "decision_tree = decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the tree as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1802e0b03d7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Change path to images folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import graphviz \n",
    "import pydot\n",
    "\n",
    "# Change path to images folder\n",
    "os.chdir(image_path)\n",
    "\n",
    "dot_data = tree.export_graphviz(decision_tree, out_file = 'tree.dot', feature_names = list(data.drop('cmedv', axis=1).columns), rounded = True, precision = 1)\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "\n",
    "# Change directory back to parent\n",
    "os.chdir(parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert this dot file, you need to use some command line magic I converted the file beforehand, so you can view the tree by running this code block.\n",
    "For anyone interested, the command line function is below (make sure you are cd'd into the random_forest/images directory and are in teh dm_ml environment)\n",
    "\n",
    "\\> *dot -Tpng tree.dot -o tree.png*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!\"images/tree.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = decision_tree.predict(X_test)\n",
    "pd.DataFrame(predictions, y_test).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# accuracy_score(np.array(y_test), np.array(predictions))\n",
    "rsquare = decision_tree.score(np.array(X_test), np.array(y_test))\n",
    "\n",
    "print(\"Our tree's R2 is {:.2} and RMSE is: \".format(rsquare, ))\n",
    "#print(np.sqrt(metrics.mean_squared_error(X_test, y_test)))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note - Please note that if we set the random states to different numbers, the result would be different, however\n",
    "While this decision tree is quite accurate, we can possibly improve accuracy using the random forest model\n",
    "The random forest model essentially builds multiple decision trees, takes the outputs from all of those trees, and \n",
    "determines the best prediction by taking the average (regression) or the mode (classification) of the outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This seems like a pretty good accuracy, except we overfit the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename= str(image_path) + 'overfitting_underfitting.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ways to combat overfitting by tuning the model. One way to do this is decrease the depth of the tree (either during or after fitting - research *pruning*). We won't get into that here, instead we will show another way to more accurately (and powerfully) predict our outcomes - the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= str(image_path) + 'random-forest.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we import, instantiate, and then fit\n",
    "# Here, n_estimators is the number of decision trees in our random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 10)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's predict and see the predictions next to the actual y values\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "pd.DataFrame(y_test, predictions).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(np.array(y_test), np.array(predictions))\n",
    "accuracy = rf.score(np.array(X_test), np.array(y_test))\n",
    "\n",
    "print(\"Our tree's accuracy is \" + str(round(accuracy*100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy improved! Let's take a deeper look at how close our model is to predicting actual housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average error for the predicted results\n",
    "absolute_errors = abs(predictions - y_test)\n",
    "mean_absolute_error = np.mean(absolute_errors)\n",
    "mean_absolute_error = round(mean_absolute_error, 4)\n",
    "\n",
    "# Remember that our housing prices are in thousands of dollars, so let's show that here\n",
    "print('Mean Absolute Error: $', 1000*mean_absolute_error, sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "os.chdir(image_path)\n",
    "\n",
    "# Export the image to a dot file\n",
    "feature_list = data.drop('cmedv', axis=1).columns\n",
    "export_graphviz(tree, out_file = 'rf_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('rf_tree.dot')\n",
    "\n",
    "os.chdir(parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(\"images/tree_from_random_forest_output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which factors of a neighborhood influence it's price the most. We can do this using a few more complex techniques in Python. I won't be getting into these and I am also going to use some code that was written by William Koehrsen in his article that can be found here: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lstat and rooms make up about 80% of the importance in predicting the housing price.<br>\n",
    "Does this make sense? It is always improtant to look at your model output and determine if it logically matches the context of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Model Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "cumulative_importances = np.round(np.cumsum(sorted_importances),2)\n",
    "\n",
    "importance_df = pd.DataFrame({'features': sorted_features, 'importance': sorted_importances, 'cumulative importance': cumulative_importances})\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pick what we want our cumulative importance to be. Here I chose 95%\n",
    "new_df = importance_df[importance_df['cumulative importance'] <= 0.95]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we only want to use these features. We can re-run the random forest with only these\n",
    "\n",
    "#split the data\n",
    "features = data[new_df.features]\n",
    "labels = data['cmedv']\n",
    "print(features.columns)\n",
    "\n",
    "#convert to numpy arrays\n",
    "import numpy as np\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 10)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# accuracy_score(np.array(y_test), np.array(predictions))\n",
    "accuracy = rf.score(np.array(X_test), np.array(y_test))\n",
    "print(\"Our tree's accuracy is \" + str(round(accuracy*100,2)) + \"%\")\n",
    "\n",
    "# Calculate the average error for the predicted results\n",
    "absolute_errors = abs(predictions - y_test)\n",
    "mean_absolute_error = np.mean(absolute_errors)\n",
    "mean_absolute_error = round(mean_absolute_error, 4)\n",
    "print('Mean Absolute Error: $', 1000*mean_absolute_error, sep = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our accuracy before was ~ 92% and that the MAE was about $2100 as well\n",
    "While our new model may seem just as good as before, remember that we dropped more than half of our features. \n",
    "That should prove that the other features really weren't adding much more value.\n",
    "This is just one example of how models do not have super complex to provide good results. What we also gained here is decreased runtime - it took less computation time to get almost the same accuracy. When scaling a model to production, this is a trade-off we would likely want to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Tuning the Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"While model parameters are learned during training — such as the slope and intercept in a linear regression — hyperparameters must be set by the data scientist before training.\" - William Koehrsen\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source <br>\n",
    "https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
    "\n",
    "Jupyter <br>\n",
    "https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html\n",
    "\n",
    "Pandas <br>\n",
    "https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python\n",
    "\n",
    "Information Gain and Entropy <br>\n",
    "https://www.saedsayad.com/decision_tree.htm <br>\n",
    "\n",
    "How to Become a Data Science <br>\n",
    "http://www.nerdgraph.com/new-infographic-become-data-scientist-8-steps/ <br>\n",
    "https://towardsdatascience.com/how-to-learn-data-science-if-youre-broke-7ecc408b53c7 <br>\n",
    "https://www.class-central.com/subject/data-science <br>\n",
    "\n",
    "General Sources & Good Reads <br>\n",
    "https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 <br>\n",
    "https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12 <br>\n",
    "https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d <br>\n",
    "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 <br>\n",
    "https://github.com/WillKoehrsen/Data-Analysis/tree/master/random_forest_explained <br>\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
